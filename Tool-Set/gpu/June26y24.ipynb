{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Nt-xwrOZpN_",
        "outputId": "ce369ec0-a542-4dae-a615-e4b57d8758ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc f7a1.cu -o a71.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HWZrUzwZqLw",
        "outputId": "6aa577df-229f-4188-aa5f-c80ed5020bcc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mf7a1.cu(36)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"final_result\"\u001b[0m was declared but never referenced\n",
            "      float final_result;\n",
            "            ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a71.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubbeOmzVZ12Q",
        "outputId": "ab92deea-3a3e-4ea1-900b-ee42ec391330"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dot product (float16): 512.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\n",
        "\\text{Dot product computation in CUDA using float16:} \\\\\n",
        "\\text{Given } \\mathbf{a}, \\mathbf{b} \\text{ are arrays of size } N, \\\\\n",
        "\\text{compute } \\sum_{i=0}^{N-1} \\text{float16}(a_i) \\times \\text{float16}(b_i)\n",
        "$"
      ],
      "metadata": {
        "id": "ZzvW3lDrl8Lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `hmul(a, b)` computes the element-wise product of arrays  \n",
        "\n",
        "$ \\mathbf{a}  $ and  $  \\mathbf{b} $,\n",
        "\n",
        "where each element is in float16 format. This operation is defined as:\n",
        "\n",
        "$ \\text{hmul}(\\mathbf{a}, \\mathbf{b}) = \\left\\{ c_i = \\text{float16}(a_i) \\times \\text{float16}(b_i) \\quad \\forall i \\right\\} $\n",
        "\n",
        "where\n",
        "$ \\mathbf{a} = [a_0, a_1, \\ldots, a_{N-1}] $\n",
        "\n",
        "and\n",
        "\n",
        "$ \\mathbf{b} = [b_0, b_1, \\ldots, b_{N-1}] $\n",
        "\n",
        "are arrays of size $ N $.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Let\n",
        "\n",
        "$ \\mathbf{a} = [1.0_{\\text{float16}}, 2.0_{\\text{float16}}, 3.0_{\\text{float16}}] $\n",
        "\n",
        " and\n",
        "\n",
        " $ \\mathbf{b} = [0.5_{\\text{float16}}, 0.5_{\\text{float16}}, 0.5_{\\text{float16}}] $.\n",
        "\n",
        "Then, the result of `hmul(a, b)` would be:\n",
        "\n",
        "$ \\text{hmul}(\\mathbf{a}, \\mathbf{b}) = [0.5_{\\text{float16}}, 1.0_{\\text{float16}}, 1.5_{\\text{float16}}] $"
      ],
      "metadata": {
        "id": "MNjJ43Bgpiu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Float16 Format\n",
        "\n",
        "Float16 (half-precision floating-point) is a 16-bit format that stores floating-point numbers. It consists of:\n",
        "- **1 sign bit**: Indicates the sign of the number (0 for positive, 1 for negative).\n",
        "- **5 exponent bits**: Represent the exponent of the number.\n",
        "- **10 fraction bits**: Represent the significand or mantissa of the number.\n",
        "\n",
        "### Polynomial Representation and Multiplication\n",
        "\n",
        "Let's represent `a` and `b` as polynomials in both float16 and float32 formats:\n",
        "\n",
        "For float16:\n",
        "- Let $ a_{\\text{float16}} $ and $ b_{\\text{float16}} $ denote the float16 representation of `a` and `b`.\n",
        "- Suppose $ a_{\\text{float16}} = \\text{sign}_a \\times 2^{\\text{exp}_a} \\times (1 + \\text{frac}_a) $ and similarly for $ b_{\\text{float16}} $.\n",
        "\n",
        "Their product $ a_{\\text{float16}} \\times b_{\\text{float16}} $ can be computed as:\n",
        "$$ a_{\\text{float16}} \\times b_{\\text{float16}} = \\text{sign}_a \\times \\text{sign}_b \\times 2^{(\\text{exp}_a + \\text{exp}_b - 15)} \\times (1 + \\text{frac}_a) \\times (1 + \\text{frac}_b) $$\n",
        "\n",
        "For float32:\n",
        "- Let $ a_{\\text{float32}} $ and $ b_{\\text{float32}} $ denote the float32 representation of `a` and `b`.\n",
        "- Suppose $ a_{\\text{float32}} = \\text{sign}_a \\times 2^{\\text{exp}_a} \\times (1 + \\text{frac}_a) $ and similarly for $ b_{\\text{float32}} $.\n",
        "\n",
        "Their product $ a_{\\text{float32}} \\times b_{\\text{float32}} $ can be computed as:\n",
        "$$ a_{\\text{float32}} \\times b_{\\text{float32}} = \\text{sign}_a \\times \\text{sign}_b \\times 2^{(\\text{exp}_a + \\text{exp}_b - 127)} \\times (1 + \\text{frac}_a) \\times (1 + \\text{frac}_b) $$\n",
        "\n",
        "### Example Calculation\n",
        "\n",
        "Let's consider an example with specific float16 and float32 values for `a` and `b`:\n",
        "\n",
        "- Suppose $ a_{\\text{float16}} = 1.5 $ and $ b_{\\text{float16}} = 0.5 $.\n",
        "- Compute $ a_{\\text{float16}} \\times b_{\\text{float16}} $ in float16 format.\n",
        "- Compute $ a_{\\text{float32}} \\times b_{\\text{float32}} $ in float32 format.\n",
        "\n",
        "### Comparison\n",
        "\n",
        "Compare the results of $ a \\times b $ in float16 and float32 formats to observe differences in precision and range.\n",
        "\n"
      ],
      "metadata": {
        "id": "FONXvA6ZvloT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For float16 representation of numbers `a = 12` and `b = 7`, we need to convert them into their respective float16 formats, including the fraction part (`frac`).\n",
        "\n",
        "### Conversion to Float16 Format\n",
        "\n",
        "#### Step 1: Represent `a` and `b` in Binary\n",
        "- **a = 12**:\n",
        "  - Binary representation: `1100`\n",
        "  - Normalize to: `1.100` (shifted left by 3 bits)\n",
        "  - Exponent: $ 3 + 15 = 18 $ (in binary: `10010`)\n",
        "  - Fraction (`frac_a`): `1000000000` (10 bits)\n",
        "  - Float16 representation: `0 10010 1000000000`\n",
        "\n",
        "- **b = 7**:\n",
        "  - Binary representation: `0111`\n",
        "  - Normalize to: `1.11` (shifted left by 2 bits)\n",
        "  - Exponent: $ 2 + 15 = 17 $ (in binary: `10001`)\n",
        "  - Fraction (`frac_b`): `1100000000` (10 bits)\n",
        "  - Float16 representation: `0 10001 1100000000`\n",
        "\n",
        "### Calculation\n",
        "\n",
        "#### Float16 Format\n",
        "- For `a`:\n",
        "  - Float16 representation: `0 10010 1000000000`\n",
        "  - Sign (`sign_a`): `0`\n",
        "  - Exponent (`exp_a`): `10010` (binary) = $ 18 $ (decimal)\n",
        "  - Fraction (`frac_a`): `1000000000`\n",
        "  - $ a_{\\text{float16}} = (-1)^{0} \\times 2^{(18-15)} \\times (1 + 0.5) = 12 $\n",
        "\n",
        "- For `b`:\n",
        "  - Float16 representation: `0 10001 1100000000`\n",
        "  - Sign (`sign_b`): `0`\n",
        "  - Exponent (`exp_b`): `10001` (binary) = $ 17 $ (decimal)\n",
        "  - Fraction (`frac_b`): `1100000000`\n",
        "  - $ b_{\\text{float16}} = (-1)^{0} \\times 2^{(17-15)} \\times (1 + 0.75) = 7 $\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Using the above representations and calculations, we can verify the float16 representations and their fractional components (`frac_a` and `frac_b`) for `a = 12` and `b = 7`. This demonstrates how these numbers are represented in float16 format, highlighting the fraction part (`frac`) used in the conversion process.\n",
        "\n",
        "To compute the product \\( a \\times b \\) using float16 representation, let's proceed with the calculation based on the provided float16 formats for \\( a = 12 \\) and \\( b = 7 \\).\n",
        "\n",
        "### Float16 Representation Recap\n",
        "\n",
        "For \\( a = 12 \\):\n",
        "- Float16 representation: `0 10010 1000000000`\n",
        "- Sign (`sign_a`): `0`\n",
        "- Exponent (`exp_a`): \\( 10010 \\) (binary) = \\( 18 \\) (decimal)\n",
        "- Fraction (`frac_a`): `1000000000`\n",
        "\n",
        "For \\( b = 7 \\):\n",
        "- Float16 representation: `0 10001 1100000000`\n",
        "- Sign (`sign_b`): `0`\n",
        "- Exponent (`exp_b`): \\( 10001 \\) (binary) = \\( 17 \\) (decimal)\n",
        "- Fraction (`frac_b`): `1100000000`\n",
        "\n",
        "### Calculation of \\( a \\times b \\)\n",
        "\n",
        "#### Step 1: Compute the Multiplication in Float16 Format\n",
        "To compute \\( a \\times b \\):\n",
        "\n",
        "1. **Sign**: Both \\( a \\) and \\( b \\) have a sign bit of `0`, indicating positive numbers.\n",
        "   \n",
        "2. **Exponent Adjustment**:\n",
        "   - Exponent of \\( a \\): \\( 18 \\) (actual exponent) - \\( 15 \\) (float16 bias) = \\( 3 \\)\n",
        "   - Exponent of \\( b \\): \\( 17 \\) (actual exponent) - \\( 15 \\) (float16 bias) = \\( 2 \\)\n",
        "   - Combined exponent for multiplication: \\( 3 + 2 = 5 \\)\n",
        "   - Adjusted exponent for float16: \\( 5 + 15 = 20 \\) (binary: \\( 10100 \\))\n",
        "\n",
        "3. **Fraction Multiplication**:\n",
        "   - Multiply the fractions (`frac_a` and `frac_b`) directly:\n",
        "     \\[\n",
        "     \\text{frac_a} \\times \\text{frac_b} = 1000000000 \\times 1100000000 = 1100000000000000000\n",
        "     \\]\n",
        "\n",
        "4. **Normalize and Round**:\n",
        "   - Normalize the fraction and adjust the exponent accordingly.\n",
        "\n",
        "### Result\n",
        "\n",
        "After performing the multiplication and adjusting for float16 format, the result would need to be rounded and normalized correctly to fit into float16 representation. The exact floating-point representation details would typically involve additional rounding and normalization steps, ensuring the result conforms to the float16 format standards.\n",
        "\n",
        "This process illustrates how multiplication is handled in float16 format, leveraging the provided details for \\( a \\) and \\( b \\).\n"
      ],
      "metadata": {
        "id": "uWhp8HhV0aK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile f7a2.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_fp16.h>\n",
        "\n",
        "#define N 512\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "__global__ void dot_product_float16(__half *a, __half *b, float *result) {\n",
        "    __shared__ float temp[BLOCK_SIZE];\n",
        "    int tid = threadIdx.x;\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Perform half-precision multiplication and accumulate in single-precision\n",
        "    temp[tid] = (index < N) ? __half2float(__hmul(a[index], b[index])) : 0.0f;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Reduce within block\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            temp[tid] += temp[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Store result of this block in global memory\n",
        "    if (tid == 0) {\n",
        "        atomicAdd(result, temp[0]);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    __half *a, *b;\n",
        "    float *result;\n",
        "    __half *d_a, *d_b;\n",
        "    float *d_result;\n",
        "    float final_result;\n",
        "\n",
        "    // Allocate host memory\n",
        "    a = (__half*)malloc(N * sizeof(__half));\n",
        "    b = (__half*)malloc(N * sizeof(__half));\n",
        "    result = (float*)malloc(sizeof(float));\n",
        "\n",
        "    // Initialize host arrays\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        a[i] = __float2half(1.0f); // Initialize to 1.0 in float16\n",
        "        b[i] = __float2half(1.0f); // Initialize to 1.0 in float16\n",
        "    }\n",
        "    *result = 0.0f;\n",
        "\n",
        "    // Allocate device memory\n",
        "    cudaMalloc((void**)&d_a, N * sizeof(__half));\n",
        "    cudaMalloc((void**)&d_b, N * sizeof(__half));\n",
        "    cudaMalloc((void**)&d_result, sizeof(float));\n",
        "\n",
        "    // Copy data from host to device\n",
        "    cudaMemcpy(d_a, a, N * sizeof(__half), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, N * sizeof(__half), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_result, result, sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel\n",
        "    dot_product_float16<<<(N + BLOCK_SIZE - 1) / BLOCK_SIZE, BLOCK_SIZE>>>(d_a, d_b, d_result);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(result, d_result, sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_result);\n",
        "\n",
        "    // Print result\n",
        "    printf(\"Dot product (float16): %f\\n\", *result);\n",
        "\n",
        "    // Free host memory\n",
        "    free(a);\n",
        "    free(b);\n",
        "    free(result);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk9m8K0xZ16c",
        "outputId": "d5d1d046-e665-4900-903d-d86150d78612"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting f7a2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc f7a2.cu -o a72.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWaTOKbHigZn",
        "outputId": "c4e240e7-5a48-4d85-fea9-1dd6091c5632"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mf7a2.cu(35)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"final_result\"\u001b[0m was declared but never referenced\n",
            "      float final_result;\n",
            "            ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./a72.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq6xhE5filQA",
        "outputId": "19171ac7-01f7-41c9-d698-67710ca8f369"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dot product (float16): 512.000000\n"
          ]
        }
      ]
    }
  ]
}